{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing pipeline 1\n",
    "def merge_rows_with_wheat_shift(df):\n",
    "    \"\"\"\n",
    "    First shifts rows where wheat appears in longitude, then merges all pairs of rows.\n",
    "    Properly preserves all values during the shifting process.\n",
    "    \"\"\"\n",
    "    processed_rows = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df) - 1:  # Process pairs of rows\n",
    "        # Get current pair of rows\n",
    "        numeric_row = df.iloc[i].copy()\n",
    "        categorical_row = df.iloc[i + 1].copy()\n",
    "        \n",
    "        # Check if this is a wheat row that needs shifting\n",
    "        if pd.notna(categorical_row['longitude']) and str(categorical_row['longitude']).strip().lower() == 'wheat':\n",
    "            # For wheat rows:\n",
    "            # 1. Move the values one column to the right\n",
    "            categorical_row['city'] = categorical_row['soil']\n",
    "            categorical_row['soil'] = categorical_row['crop']\n",
    "            categorical_row['crop'] = 'WHEAT'  # Set crop to WHEAT\n",
    "            \n",
    "            # 2. Keep the numeric longitude from the first row if it exists\n",
    "            if pd.isna(numeric_row['longitude']):\n",
    "                numeric_row['longitude'] = np.nan  # Set as missing value when no longitude exists\n",
    "        else:\n",
    "            # For non-wheat rows:\n",
    "            # If there's a longitude in the categorical row, use it\n",
    "            if pd.notna(categorical_row['longitude']):\n",
    "                numeric_row['longitude'] = categorical_row['longitude']\n",
    "        \n",
    "        # Merge categorical values into the numeric row\n",
    "        categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "        for col in categorical_cols:\n",
    "            if pd.notna(categorical_row[col]):\n",
    "                numeric_row[col] = categorical_row[col].strip()\n",
    "        \n",
    "        processed_rows.append(numeric_row)\n",
    "        i += 2\n",
    "    \n",
    "    # If there's a lone last row, add it\n",
    "    if i == len(df) - 1:\n",
    "        processed_rows.append(df.iloc[-1])\n",
    "    \n",
    "    result_df = pd.DataFrame(processed_rows, columns=df.columns)\n",
    "    \n",
    "    # Convert longitude to numeric, replacing any remaining NaN with mean\n",
    "    result_df['longitude'] = pd.to_numeric(result_df['longitude'], errors='coerce')\n",
    "    result_df['longitude'] = result_df['longitude'].fillna(result_df['longitude'].mean())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def clean_categorical_values(df):\n",
    "    \"\"\"Standardizes categorical values by converting to uppercase and removing extra spaces.\"\"\"\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles missing values in both numeric and categorical columns.\"\"\"\n",
    "    # For numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Creates new features from existing data.\"\"\"\n",
    "    # Temperature ratio (with handling for zero values)\n",
    "    df['temp_ratio'] = df['Max Temp'] / df['Min Temp'].replace(0, np.nan)\n",
    "    df['temp_ratio'] = df['temp_ratio'].fillna(df['temp_ratio'].median())\n",
    "    \n",
    "    # Humidity/radiation ratio\n",
    "    df['humidity_rad_ratio'] = df['Humidity'] / df['Rad'].replace(0, np.nan)\n",
    "    df['humidity_rad_ratio'] = df['humidity_rad_ratio'].fillna(df['humidity_rad_ratio'].median())\n",
    "    \n",
    "    # Season mapping\n",
    "    season_mapping = {\n",
    "        'DECEMBER': 'WINTER', 'JANUARY': 'WINTER', 'FEBRUARY': 'WINTER',\n",
    "        'MARCH': 'SPRING', 'APRIL': 'SPRING', 'MAY': 'SPRING',\n",
    "        'JUNE': 'SUMMER', 'JULY': 'SUMMER', 'AUGUST': 'SUMMER',\n",
    "        'SEPTEMBER': 'AUTUMN', 'OCTOBER': 'AUTUMN', 'NOVEMBER': 'AUTUMN'\n",
    "    }\n",
    "    df['season'] = df['month'].map(season_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_and_encode(df):\n",
    "    \"\"\"Normalizes numeric features and encodes categorical variables.\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Normalize numeric columns\n",
    "    numeric_cols = ['water req', 'Min Temp', 'Max Temp', 'Humidity', 'Wind', \n",
    "                   'Sun', 'Rad', 'Rain', 'altitude', 'latitude', 'longitude',\n",
    "                   'temp_ratio', 'humidity_rad_ratio']\n",
    "    \n",
    "    # Create and fit scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city', 'season']\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_encoded[col] = encoders[col].fit_transform(df_encoded[col])\n",
    "    \n",
    "    return df_encoded, encoders\n",
    "\n",
    "def process_dataset1(input_file, output_dir):\n",
    "    \"\"\"Main function to process the dataset.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Apply the processing pipeline\n",
    "    df = merge_rows_with_wheat_shift(df)\n",
    "    df = clean_categorical_values(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Save the preprocessed but non-normalized version\n",
    "    df.to_csv(os.path.join(output_dir, 'dataset_1_preprocessed.csv'), index=False)\n",
    "    \n",
    "    # Create normalized version\n",
    "    df_normalized, encoders = normalize_and_encode(df)\n",
    "    \n",
    "    # Save the normalized version\n",
    "    df_normalized.to_csv(os.path.join(output_dir, 'dataset_1_normalized.csv'), index=False)\n",
    "    \n",
    "    return df, df_normalized, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du pipeline 1\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = r\"Datasets2\\dataset1\\data_plants.csv\" \n",
    "    \n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    # Unpack all three returned values\n",
    "    df_raw, df_normalized, encoders = process_dataset1(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing pipeline 2\n",
    "def convert_timestamp(df):\n",
    "    \"\"\"Converts Unix timestamps to datetime and extracts temporal features.\"\"\"\n",
    "    # Convert Unix timestamp to datetime\n",
    "    df['datetime'] = pd.to_datetime(df['time'], unit='s')\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Drop original time column and datetime (keep date as string)\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    df = df.drop(['time', 'datetime'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_hour(df):\n",
    "    \"\"\"Categorizes hours into periods of the day.\"\"\"\n",
    "    conditions = [\n",
    "        (df['hour'] >= 5) & (df['hour'] < 12),\n",
    "        (df['hour'] >= 12) & (df['hour'] < 17),\n",
    "        (df['hour'] >= 17) & (df['hour'] < 21),\n",
    "        (df['hour'] >= 21) | (df['hour'] < 5)\n",
    "    ]\n",
    "    periods = ['MORNING', 'AFTERNOON', 'EVENING', 'NIGHT']\n",
    "    \n",
    "    df['day_period'] = np.select(conditions, periods, default='UNKNOWN')\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles any missing values in the dataset.\"\"\"\n",
    "    # For numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns (if any were created)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_and_encode(df):\n",
    "    \"\"\"Normalizes numeric features while preserving categorical ones.\"\"\"\n",
    "    # Identify numeric columns to normalize\n",
    "    numeric_cols = ['water', 'hour']\n",
    "    \n",
    "    # Create and fit scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # encode categorical columns\n",
    "    categorical_cols = ['day_period']\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df[col] = encoders[col].fit_transform(df[col])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def process_dataset2(input_file, output_dir):\n",
    "    \"\"\"Main function to process the tomato dataset.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Try to read first few lines to check for header\n",
    "        with open(input_file, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        \n",
    "        # If first line contains header-like content, skip it\n",
    "        if 'time' in first_line.lower() or 'simulation' in first_line.lower():\n",
    "            df = pd.read_csv(input_file, skiprows=1, names=['simulation_id', 'time', 'water', 'hour'])\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, names=['simulation_id', 'time', 'water', 'hour'])\n",
    "        \n",
    "        # Apply the processing pipeline\n",
    "        df = convert_timestamp(df)\n",
    "        df = categorize_hour(df)\n",
    "        df = handle_missing_values(df)\n",
    "        \n",
    "        # Save the preprocessed but non-normalized version\n",
    "        df.to_csv(os.path.join(output_dir, 'dataset_2_preprocessed.csv'), index=False)\n",
    "        \n",
    "        # Create normalized version\n",
    "        df_normalized, scaler = normalize_and_encode(df)\n",
    "        \n",
    "        # Save the normalized version\n",
    "        df_normalized.to_csv(os.path.join(output_dir, 'dataset_2_normalized.csv'), index=False)\n",
    "        \n",
    "        return df, df_normalized, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du pipeline 2\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset2/tomates.csv\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    # Process the dataset\n",
    "    df_raw, df_normalized, scaler = process_dataset2(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "REQUIRED_COLUMNS = {\n",
    "    'growing_period': ['Crop', 'Total growing period (days)'],\n",
    "    'water_need': ['Crop', 'Crop water need (mm/total growing period)']\n",
    "}\n",
    "\n",
    "def split_range(range_str: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Split a range string formatted as \"min-max\" into min and max values as floats.\n",
    "    \n",
    "    Args:\n",
    "        range_str: String containing the range in format \"min-max\"\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (min_value, max_value) as floats. Returns (NaN, NaN) if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle various formats and clean the input\n",
    "        range_str = str(range_str).strip().replace(' ', '')\n",
    "        \n",
    "        # Handle single values\n",
    "        if range_str.replace('.', '').isdigit():\n",
    "            value = float(range_str)\n",
    "            return value, value\n",
    "            \n",
    "        # Handle range values\n",
    "        if '-' in range_str:\n",
    "            parts = range_str.split('-')\n",
    "            if len(parts) == 2:\n",
    "                min_val = float(parts[0])\n",
    "                max_val = float(parts[1])\n",
    "                # Ensure min <= max\n",
    "                if min_val <= max_val:\n",
    "                    return min_val, max_val\n",
    "                \n",
    "        return np.nan, np.nan\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing range '{range_str}': {str(e)}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def process_ranges(df: pd.DataFrame, col: str, new_min_col: str, new_max_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a column containing range strings into separate min and max columns.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        logging.error(f\"Column '{col}' not found in DataFrame\")\n",
    "        return df\n",
    "        \n",
    "    ranges = df[col].apply(split_range)\n",
    "    df[new_min_col] = ranges.apply(lambda x: x[0])\n",
    "    df[new_max_col] = ranges.apply(lambda x: x[1])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def expand_crop_names(crop_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split compound crop names into individual crops.\n",
    "    \n",
    "    Args:\n",
    "        crop_name: String containing possibly multiple crop names separated by '/'\n",
    "        \n",
    "    Returns:\n",
    "        List of individual crop names\n",
    "    \"\"\"\n",
    "    return [name.strip().upper() for name in crop_name.split('/')]\n",
    "\n",
    "def process_dataset3(input_file: str, output_dir: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the FAO Website_data.xls dataset with improved crop name handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input file\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "            \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Try reading as tab-delimited file first\n",
    "        logging.info(f\"Reading input file as tab-delimited: {input_file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, sep='\\t')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to read as tab-delimited, trying Excel format: {str(e)}\")\n",
    "            df = pd.read_excel(input_file, engine='xlrd')\n",
    "        \n",
    "        # Split the dataframe into two parts based on non-null values\n",
    "        df_period = df[['Crop', 'Total growing period (days)']].dropna(subset=['Total growing period (days)'])\n",
    "        df_water = df[['Crop', 'Crop water need (mm/total growing period)']].dropna(subset=['Crop water need (mm/total growing period)'])\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        df_period = df_period.rename(columns={\n",
    "            'Total growing period (days)': 'Total_growing_period'\n",
    "        })\n",
    "        \n",
    "        df_water = df_water.rename(columns={\n",
    "            'Crop water need (mm/total growing period)': 'Crop_water_need'\n",
    "        })\n",
    "        \n",
    "        # Clean crop names\n",
    "        df_period['Crop'] = df_period['Crop'].str.strip().str.upper()\n",
    "        df_water['Crop'] = df_water['Crop'].str.strip().str.upper()\n",
    "        \n",
    "        # Expand compound crop names\n",
    "        period_rows = []\n",
    "        for _, row in df_period.iterrows():\n",
    "            for crop in expand_crop_names(row['Crop']):\n",
    "                new_row = row.copy()\n",
    "                new_row['Crop'] = crop\n",
    "                period_rows.append(new_row)\n",
    "        df_period = pd.DataFrame(period_rows)\n",
    "        \n",
    "        water_rows = []\n",
    "        for _, row in df_water.iterrows():\n",
    "            for crop in expand_crop_names(row['Crop']):\n",
    "                new_row = row.copy()\n",
    "                new_row['Crop'] = crop\n",
    "                water_rows.append(new_row)\n",
    "        df_water = pd.DataFrame(water_rows)\n",
    "        \n",
    "        # Process range values for both dataframes\n",
    "        df_period = process_ranges(\n",
    "            df_period,\n",
    "            'Total_growing_period',\n",
    "            'Total_growing_period_min',\n",
    "            'Total_growing_period_max'\n",
    "        )\n",
    "        df_period = df_period.drop('Total_growing_period', axis=1)\n",
    "        \n",
    "        df_water = process_ranges(\n",
    "            df_water,\n",
    "            'Crop_water_need',\n",
    "            'Crop_water_need_min',\n",
    "            'Crop_water_need_max'\n",
    "        )\n",
    "        df_water = df_water.drop('Crop_water_need', axis=1)\n",
    "        \n",
    "        # Calculate averages for both dataframes\n",
    "        df_period['Total_growing_period_avg'] = df_period[\n",
    "            ['Total_growing_period_min', 'Total_growing_period_max']\n",
    "        ].mean(axis=1)\n",
    "        \n",
    "        df_water['Crop_water_need_avg'] = df_water[\n",
    "            ['Crop_water_need_min', 'Crop_water_need_max']\n",
    "        ].mean(axis=1)\n",
    "        \n",
    "        # Merge the dataframes on Crop name\n",
    "        df_combined = pd.merge(\n",
    "            df_period,\n",
    "            df_water,\n",
    "            on='Crop',\n",
    "            how='outer'\n",
    "        )\n",
    "        \n",
    "        # Sort by crop name for better readability\n",
    "        df_combined = df_combined.sort_values('Crop')\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, 'dataset_3_processed.csv')\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        return df_combined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 18:02:18,848 - INFO - Reading input file as tab-delimited: Datasets2/dataset3/Website_data.xls\n",
      "2025-02-21 18:02:18,891 - INFO - Processed data saved to: Output\\dataset_3_processed.csv\n",
      "2025-02-21 18:02:18,893 - INFO - Dataset processing completed successfully\n",
      "2025-02-21 18:02:18,895 - INFO - Processed 40 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset3/Website_data.xls\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    try:\n",
    "        df_processed = process_dataset3(input_file, output_dir)\n",
    "        if df_processed is not None:\n",
    "            logging.info(\"Dataset processing completed successfully\")\n",
    "            logging.info(f\"Processed {len(df_processed)} records\")\n",
    "        else:\n",
    "            logging.error(\"Dataset processing failed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "CROP_TYPE_MAPPING = {\n",
    "    1: 'Paddy',\n",
    "    2: 'Ground Nuts'\n",
    "}\n",
    "\n",
    "def validate_data(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the input dataframe structure and content.\n",
    "    \"\"\"\n",
    "    required_columns = [\n",
    "        'CropType', 'CropDays', 'Soil Moisture', 'Soil Temperature',\n",
    "        'Temperature', 'Humidity', 'Irrigation(Y/N)'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Check for required columns\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                logging.error(f\"Missing required column: {col}\")\n",
    "                return False\n",
    "        \n",
    "        # Validate data types\n",
    "        if not df['CropType'].dtype in ['int64', 'float64']:\n",
    "            logging.error(\"CropType column should contain numeric values\")\n",
    "            return False\n",
    "            \n",
    "        if not df['Irrigation(Y/N)'].dtype in ['int64', 'float64']:\n",
    "            logging.error(\"Irrigation column should contain numeric values\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_dataset4(input_file: str, output_dir: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the irrigation dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input file\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "            \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read only the main data part (first 7 columns)\n",
    "        logging.info(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_excel(input_file, usecols=range(7))\n",
    "        \n",
    "        # Validate data structure\n",
    "        if not validate_data(df):\n",
    "            raise ValueError(\"Data validation failed\")\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Convert crop types to names\n",
    "        df['CropType'] = df['CropType'].map(CROP_TYPE_MAPPING)\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, 'dataset_4_processed.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 22:23:17,446 - INFO - Reading input file: Datasets2/dataset4/Project_datasheet_2019-2020.xlsx\n",
      "2025-02-21 22:23:17,491 - INFO - Processed data saved to: Output\\dataset_4_processed.csv\n",
      "2025-02-21 22:23:17,491 - INFO - Dataset processing completed successfully\n",
      "2025-02-21 22:23:17,497 - INFO - Processed 150 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset4/Project_datasheet_2019-2020.xlsx\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    try:\n",
    "        df_processed = process_dataset4(input_file, output_dir)\n",
    "        if df_processed is not None:\n",
    "            logging.info(\"Dataset processing completed successfully\")\n",
    "            logging.info(f\"Processed {len(df_processed)} records\")\n",
    "        else:\n",
    "            logging.error(\"Dataset processing failed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert month to season.\n",
    "    \"\"\"\n",
    "    month = month.lower()\n",
    "    seasons = {\n",
    "        'winter': ['december', 'january', 'february'],\n",
    "        'spring': ['march', 'april', 'may'],\n",
    "        'summer': ['june', 'july', 'august'],\n",
    "        'autumn': ['september', 'october', 'november']\n",
    "    }\n",
    "    \n",
    "    for season, months in seasons.items():\n",
    "        if month in months:\n",
    "            return season.upper()\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "def read_and_process_file(file_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read and process a single data file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine file type and read accordingly\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            logging.error(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Standardize column names to lowercase\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_dataset6(input_dir: str, output_dir: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the agricultural dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define input files\n",
    "        files = {\n",
    "            'ble': 'blé.csv',\n",
    "            'riz': 'le riz2.csv',\n",
    "            'mais': 'maïs.xlsx',\n",
    "            'potato': 'potato.csv'\n",
    "        }\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read and combine all files\n",
    "        dataframes = []\n",
    "        for name, file in files.items():\n",
    "            file_path = os.path.join(input_dir, file)\n",
    "            df = read_and_process_file(file_path)\n",
    "            if df is not None:\n",
    "                dataframes.append(df)\n",
    "            else:\n",
    "                logging.error(f\"Failed to process {file}\")\n",
    "                return None\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        df_combined = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Standardize column names and remove duplicates\n",
    "        df_combined.columns = df_combined.columns.str.lower().str.strip()\n",
    "        \n",
    "        # Keep only unique columns\n",
    "        df_combined = df_combined.loc[:, ~df_combined.columns.duplicated()]\n",
    "        \n",
    "        # Convert categorical fields to uppercase\n",
    "        categorical_columns = ['month', 'crop', 'soil', 'city']\n",
    "        for col in categorical_columns:\n",
    "            df_combined[col] = df_combined[col].str.upper()\n",
    "        \n",
    "        # Convert numeric columns to appropriate types\n",
    "        numeric_columns = ['water req', 'min temp', 'max temp', 'humidity', 'wind', \n",
    "                         'sun', 'rad', 'rain', 'altitude', 'latitude', 'longitude']\n",
    "        for col in numeric_columns:\n",
    "            df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
    "        \n",
    "        # Add new features\n",
    "        # 1. Temperature ratio\n",
    "        df_combined['temp_ratio'] = df_combined['max temp'] / df_combined['min temp']\n",
    "        \n",
    "        # 2. Humidity-radiation ratio\n",
    "        df_combined['humidity_rad_ratio'] = df_combined['humidity'] / df_combined['rad']\n",
    "        \n",
    "        # 3. Season\n",
    "        df_combined['season'] = df_combined['month'].apply(get_season)\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        columns_order = [\n",
    "            'water req', 'month', 'min temp', 'max temp', 'humidity', 'wind', \n",
    "            'sun', 'rad', 'rain', 'altitude', 'latitude', 'longitude',\n",
    "            'crop', 'soil', 'city', 'temp_ratio', 'humidity_rad_ratio', 'season'\n",
    "        ]\n",
    "        df_combined = df_combined[columns_order]\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, 'dataset_6_processed.csv')\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        return df_combined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 00:06:27,640 - INFO - Processed data saved to: Output\\dataset_6_processed.csv\n",
      "2025-02-22 00:06:27,641 - INFO - Dataset processing completed successfully\n",
      "2025-02-22 00:06:27,642 - INFO - Processed 115 records\n",
      "2025-02-22 00:06:27,644 - INFO - \n",
      "Sample of processed data:\n",
      "2025-02-22 00:06:27,645 - INFO -    water req  month  min temp  max temp  humidity  wind  sun   rad   rain  \\\n",
      "0       46.8  MARCH      16.0      32.0        35   192  8.0  19.2    5.0   \n",
      "1      181.3  APRIL      22.0      37.0        26   240  9.0  22.6    4.0   \n",
      "2      327.5    MAY      26.0      41.0        27   288  9.0  23.4   18.0   \n",
      "3      187.5   JUNE      27.0      39.0        46   312  7.0  20.6   51.0   \n",
      "4        0.0   JULY      25.0      34.0        73   264  5.0  17.5  213.0   \n",
      "\n",
      "   altitude  latitude  longitude   crop       soil    city  temp_ratio  \\\n",
      "0       431     26.91        NaN  WHEAT  RED SANDY  JAIPUR    2.000000   \n",
      "1       431     26.91        NaN  WHEAT  RED SANDY  JAIPUR    1.681818   \n",
      "2       431     26.91        NaN  WHEAT  RED SANDY  JAIPUR    1.576923   \n",
      "3       431     26.91        NaN  WHEAT  RED SANDY  JAIPUR    1.444444   \n",
      "4       431     26.91        NaN  WHEAT  RED SANDY  JAIPUR    1.360000   \n",
      "\n",
      "   humidity_rad_ratio  season  \n",
      "0            1.822917  SPRING  \n",
      "1            1.150442  SPRING  \n",
      "2            1.153846  SPRING  \n",
      "3            2.233010  SUMMER  \n",
      "4            4.171429  SUMMER  \n",
      "2025-02-22 00:06:27,655 - INFO - \n",
      "Column information:\n",
      "2025-02-22 00:06:27,659 - INFO - water req: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,660 - INFO - month: object (null values: 0)\n",
      "2025-02-22 00:06:27,662 - INFO - min temp: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,664 - INFO - max temp: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,665 - INFO - humidity: int64 (null values: 0)\n",
      "2025-02-22 00:06:27,667 - INFO - wind: int64 (null values: 0)\n",
      "2025-02-22 00:06:27,669 - INFO - sun: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,672 - INFO - rad: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,674 - INFO - rain: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,676 - INFO - altitude: int64 (null values: 0)\n",
      "2025-02-22 00:06:27,679 - INFO - latitude: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,681 - INFO - longitude: float64 (null values: 30)\n",
      "2025-02-22 00:06:27,683 - INFO - crop: object (null values: 0)\n",
      "2025-02-22 00:06:27,685 - INFO - soil: object (null values: 0)\n",
      "2025-02-22 00:06:27,688 - INFO - city: object (null values: 0)\n",
      "2025-02-22 00:06:27,689 - INFO - temp_ratio: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,689 - INFO - humidity_rad_ratio: float64 (null values: 0)\n",
      "2025-02-22 00:06:27,689 - INFO - season: object (null values: 0)\n"
     ]
    }
   ],
   "source": [
    "# Utilisation du pipeline 6\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = r\"Datasets2/Dataset6/riz+mais+patate+ble\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    try:\n",
    "        df_processed = process_dataset6(input_dir, output_dir)\n",
    "        if df_processed is not None:\n",
    "            logging.info(\"Dataset processing completed successfully\")\n",
    "            logging.info(f\"Processed {len(df_processed)} records\")\n",
    "            \n",
    "            # Print sample of processed data\n",
    "            logging.info(\"\\nSample of processed data:\")\n",
    "            logging.info(df_processed.head())\n",
    "            \n",
    "            # Print data quality information\n",
    "            logging.info(\"\\nColumn information:\")\n",
    "            for col in df_processed.columns:\n",
    "                null_count = df_processed[col].isnull().sum()\n",
    "                logging.info(f\"{col}: {df_processed[col].dtype} (null values: {null_count})\")\n",
    "        else:\n",
    "            logging.error(\"Dataset processing failed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load and display initial information about datasets\"\"\"\n",
    "    df1 = pd.read_csv('Output/dataset_1_preprocessed.csv')\n",
    "    df6 = pd.read_csv('Output/dataset_6_processed.csv')\n",
    "    df3 = pd.read_csv('Output/dataset_3_processed.csv')\n",
    "    \n",
    "    print(\"\\nInitial Dataset Information:\")\n",
    "    print(\"Dataset 1 shape:\", df1.shape)\n",
    "    print(\"Dataset 6 shape:\", df6.shape)\n",
    "    print(\"Dataset 3 shape:\", df3.shape)\n",
    "    \n",
    "    return df1, df6, df3\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    \"\"\"Standardize column names to lowercase and replace spaces with underscores\"\"\"\n",
    "    return df.rename(columns=lambda x: x.lower().replace(' ', '_'))\n",
    "\n",
    "def clean_and_standardize(df1, df6):\n",
    "    \"\"\"Clean and combine datasets 1 and 6\"\"\"\n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    df1 = df1.copy()\n",
    "    df6 = df6.copy()\n",
    "    \n",
    "    # Standardize column names\n",
    "    df1 = standardize_column_names(df1)\n",
    "    df6 = standardize_column_names(df6)\n",
    "    \n",
    "    # Ensure column names match exactly before concatenation\n",
    "    common_columns = list(set(df1.columns) & set(df6.columns))\n",
    "    df1 = df1[common_columns]\n",
    "    df6 = df6[common_columns]\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([df1, df6], ignore_index=True)\n",
    "    \n",
    "    # Convert month and crop to uppercase\n",
    "    if 'month' in combined_df.columns:\n",
    "        combined_df['month'] = combined_df['month'].str.upper()\n",
    "    if 'crop' in combined_df.columns:\n",
    "        combined_df['crop'] = combined_df['crop'].str.upper()\n",
    "    \n",
    "    print(\"\\nCombined Dataset Information:\")\n",
    "    print(\"Shape after combination:\", combined_df.shape)\n",
    "    print(\"Columns:\", combined_df.columns.tolist())\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def process_dataset3(df3):\n",
    "    \"\"\"Process dataset 3 for merging\"\"\"\n",
    "    df3 = df3.copy()\n",
    "    # Standardize column names\n",
    "    df3 = standardize_column_names(df3)\n",
    "    \n",
    "    # Convert crop to uppercase\n",
    "    if 'crop' in df3.columns:\n",
    "        df3['crop'] = df3['crop'].str.upper()\n",
    "    \n",
    "    print(\"\\nDataset 3 Processing Information:\")\n",
    "    print(\"Columns after processing:\", df3.columns.tolist())\n",
    "    \n",
    "    return df3\n",
    "\n",
    "def merge_datasets(combined_df, df3):\n",
    "    \"\"\"Merge the combined dataset with dataset 3\"\"\"\n",
    "    # Ensure 'crop' column exists in both dataframes\n",
    "    if 'crop' not in combined_df.columns or 'crop' not in df3.columns:\n",
    "        raise ValueError(\"'crop' column missing from one or both datasets\")\n",
    "    \n",
    "    # Merge datasets\n",
    "    final_df = pd.merge(\n",
    "        combined_df,\n",
    "        df3,\n",
    "        how='left',\n",
    "        on='crop'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMerge Information:\")\n",
    "    print(\"Shape after merge:\", final_df.shape)\n",
    "    print(\"Columns after merge:\", final_df.columns.tolist())\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def select_features(df):\n",
    "    \"\"\"Select and verify relevant features\"\"\"\n",
    "    # Define desired features\n",
    "    selected_features = [\n",
    "        'water_req',  # target variable\n",
    "        'month',\n",
    "        'min_temp',\n",
    "        'max_temp',\n",
    "        'humidity',\n",
    "        'wind',\n",
    "        'sun',\n",
    "        'rad',\n",
    "        'rain',\n",
    "        'altitude',\n",
    "        'latityde',\n",
    "        'longitude',\n",
    "        'soil',\n",
    "        'city',\n",
    "        'temp_ratio',\n",
    "        'humidity_rad_ratio',\n",
    "        'season',\n",
    "        'crop',\n",
    "        'total_growing_period_avg',\n",
    "        'crop_water_need_min',\n",
    "        'crop_water_need_max',\n",
    "        'crop_water_need_avg'\n",
    "    ]\n",
    "    \n",
    "    # Check which features are available\n",
    "    available_features = [feat for feat in selected_features if feat in df.columns]\n",
    "    missing_features = [feat for feat in selected_features if feat not in df.columns]\n",
    "    \n",
    "    print(\"\\nFeature Selection Information:\")\n",
    "    print(\"Available features:\", available_features)\n",
    "    print(\"Missing features:\", missing_features)\n",
    "    \n",
    "    # Select only available features\n",
    "    final_df = df[available_features]\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# def clean_final_dataset(df):\n",
    "#     \"\"\"Perform final cleaning on the dataset\"\"\"\n",
    "#     # Drop rows with missing values\n",
    "#     df_cleaned = df.dropna()\n",
    "    \n",
    "#     # Convert categorical variables to uppercase\n",
    "#     categorical_cols = ['month', 'soil', 'season', 'crop']\n",
    "#     for col in categorical_cols:\n",
    "#         if col in df_cleaned.columns:\n",
    "#             df_cleaned[col] = df_cleaned[col].str.upper()\n",
    "    \n",
    "#     print(\"\\nFinal Cleaning Information:\")\n",
    "#     print(\"Shape before cleaning:\", df.shape)\n",
    "#     print(\"Shape after cleaning:\", df_cleaned.shape)\n",
    "    \n",
    "#     return df_cleaned\n",
    "\n",
    "def process_join_datasets():\n",
    "    \"\"\"Main function to process and join all datasets\"\"\"\n",
    "    try:\n",
    "        # Load datasets\n",
    "        print(\"Loading datasets...\")\n",
    "        df1, df6, df3 = load_datasets()\n",
    "        \n",
    "        # Clean and combine datasets 1 and 6\n",
    "        print(\"\\nCleaning and standardizing datasets...\")\n",
    "        combined_df = clean_and_standardize(df1, df6)\n",
    "        \n",
    "        # Process dataset 3\n",
    "        print(\"\\nProcessing dataset 3...\")\n",
    "        df3_processed = process_dataset3(df3)\n",
    "        \n",
    "        # Merge datasets\n",
    "        print(\"\\nMerging datasets...\")\n",
    "        merged_df = merge_datasets(combined_df, df3_processed)\n",
    "        \n",
    "        # Select features\n",
    "        print(\"\\nSelecting relevant features...\")\n",
    "        final_df = select_features(merged_df)\n",
    "        \n",
    "        # Final cleaning\n",
    "        # print(\"\\nPerforming final cleaning...\")\n",
    "        # final_df = clean_final_dataset(final_df)\n",
    "        \n",
    "        # Save the final dataset\n",
    "        output_path = 'Output/dataset_final_joined.csv'\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nProcess completed. Dataset saved as: {output_path}\")\n",
    "        \n",
    "        # Print final statistics\n",
    "        print(\"\\nFinal Dataset Statistics:\")\n",
    "        print(f\"Total number of rows: {len(final_df)}\")\n",
    "        print(f\"Number of unique crops: {final_df['crop'].nunique()}\")\n",
    "        print(f\"Final columns: {final_df.columns.tolist()}\")\n",
    "        print(\"\\nSample of final dataset:\")\n",
    "        print(final_df.head())\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Initial Dataset Information:\n",
      "Dataset 1 shape: (1910, 18)\n",
      "Dataset 6 shape: (115, 18)\n",
      "Dataset 3 shape: (40, 7)\n",
      "\n",
      "Cleaning and standardizing datasets...\n",
      "\n",
      "Combined Dataset Information:\n",
      "Shape after combination: (2025, 18)\n",
      "Columns: ['longitude', 'humidity_rad_ratio', 'season', 'humidity', 'water_req', 'latitude', 'soil', 'rain', 'rad', 'month', 'crop', 'min_temp', 'wind', 'max_temp', 'temp_ratio', 'city', 'sun', 'altitude']\n",
      "\n",
      "Processing dataset 3...\n",
      "\n",
      "Dataset 3 Processing Information:\n",
      "Columns after processing: ['crop', 'total_growing_period_min', 'total_growing_period_max', 'total_growing_period_avg', 'crop_water_need_min', 'crop_water_need_max', 'crop_water_need_avg']\n",
      "\n",
      "Merging datasets...\n",
      "\n",
      "Merge Information:\n",
      "Shape after merge: (2025, 24)\n",
      "Columns after merge: ['longitude', 'humidity_rad_ratio', 'season', 'humidity', 'water_req', 'latitude', 'soil', 'rain', 'rad', 'month', 'crop', 'min_temp', 'wind', 'max_temp', 'temp_ratio', 'city', 'sun', 'altitude', 'total_growing_period_min', 'total_growing_period_max', 'total_growing_period_avg', 'crop_water_need_min', 'crop_water_need_max', 'crop_water_need_avg']\n",
      "\n",
      "Selecting relevant features...\n",
      "\n",
      "Feature Selection Information:\n",
      "Available features: ['water_req', 'month', 'min_temp', 'max_temp', 'humidity', 'wind', 'sun', 'rad', 'rain', 'altitude', 'longitude', 'soil', 'city', 'temp_ratio', 'humidity_rad_ratio', 'season', 'crop', 'total_growing_period_avg', 'crop_water_need_min', 'crop_water_need_max', 'crop_water_need_avg']\n",
      "Missing features: ['latityde']\n",
      "\n",
      "Process completed. Dataset saved as: Output/dataset_final_joined.csv\n",
      "\n",
      "Final Dataset Statistics:\n",
      "Total number of rows: 2025\n",
      "Number of unique crops: 5\n",
      "Final columns: ['water_req', 'month', 'min_temp', 'max_temp', 'humidity', 'wind', 'sun', 'rad', 'rain', 'altitude', 'longitude', 'soil', 'city', 'temp_ratio', 'humidity_rad_ratio', 'season', 'crop', 'total_growing_period_avg', 'crop_water_need_min', 'crop_water_need_max', 'crop_water_need_avg']\n",
      "\n",
      "Sample of final dataset:\n",
      "    water_req  month  min_temp  max_temp  humidity    wind    sun    rad  \\\n",
      "0   83.990001  MARCH     16.08     32.01     35.03  192.02   8.10  19.24   \n",
      "1   39.070000  MARCH     21.02     32.03     62.01  168.09   9.02  22.08   \n",
      "2  235.180000    MAY     27.07     39.02     63.07  312.01   9.10  23.27   \n",
      "3  108.980000  APRIL     21.05     36.03     41.07  168.05  10.09  23.84   \n",
      "4    0.020000   JULY     26.01     36.02     64.07  288.04   6.07  18.43   \n",
      "\n",
      "     rain  altitude  ...       soil     city temp_ratio  humidity_rad_ratio  \\\n",
      "0    5.04    431.07  ...  RED LOAMY   JAIPUR   1.990672            1.820686   \n",
      "1   10.00     14.05  ...  RED SANDY   MUMBAI   1.523787            2.808424   \n",
      "2   32.08      7.00  ...  RED LOAMY  CHENNAI   1.441448            2.710357   \n",
      "3   40.07    216.07  ...  RED SANDY    DELHI   1.711639            1.722735   \n",
      "4  111.01      7.08  ...  RED SANDY  CHENNAI   1.384852            3.476397   \n",
      "\n",
      "   season    crop total_growing_period_avg  crop_water_need_min  \\\n",
      "0  SPRING  POTATO                    125.0                  NaN   \n",
      "1  SPRING   WHEAT                    135.0                450.0   \n",
      "2  SPRING  POTATO                    125.0                  NaN   \n",
      "3  SPRING   WHEAT                    135.0                450.0   \n",
      "4  SUMMER   WHEAT                    135.0                450.0   \n",
      "\n",
      "   crop_water_need_max  crop_water_need_avg  \n",
      "0                  NaN                  NaN  \n",
      "1                650.0                550.0  \n",
      "2                  NaN                  NaN  \n",
      "3                650.0                550.0  \n",
      "4                650.0                550.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# # Execute the processing\n",
    "# if __name__ == \"__main__\":\n",
    "final_df = process_join_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "water_req                      0\n",
       "month                          0\n",
       "min_temp                       0\n",
       "max_temp                       0\n",
       "humidity                       0\n",
       "wind                           0\n",
       "sun                            0\n",
       "rad                            0\n",
       "rain                           0\n",
       "altitude                       0\n",
       "longitude                     30\n",
       "soil                           0\n",
       "city                           0\n",
       "temp_ratio                     0\n",
       "humidity_rad_ratio             0\n",
       "season                         0\n",
       "crop                           0\n",
       "total_growing_period_avg      27\n",
       "crop_water_need_min         1330\n",
       "crop_water_need_max         1330\n",
       "crop_water_need_avg         1330\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data final preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset with encoding and normalization\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop specified columns\n",
    "    df = df.drop(['crop_water_need_min', 'crop_water_need_max', 'crop_water_need_avg'], axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['longitude'] = df['longitude'].fillna(df['longitude'].median())\n",
    "    df['total_growing_period_avg'] = df['total_growing_period_avg'].fillna(df['total_growing_period_avg'].median())\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    categorical_columns = ['month', 'soil', 'city', 'season', 'crop']\n",
    "    numerical_columns = ['min_temp', 'max_temp', 'humidity', 'wind', 'sun', 'rad', 'rain', \n",
    "                        'altitude', 'longitude', 'temp_ratio', 'humidity_rad_ratio', \n",
    "                        'total_growing_period_avg']\n",
    "    \n",
    "    # Initialize dictionary to store encoders\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for column in categorical_columns:\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        df[column] = label_encoders[column].fit_transform(df[column])\n",
    "    \n",
    "    # Create scaled version of numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('water_req', axis=1)\n",
    "    y = df['water_req']\n",
    "    \n",
    "    # Scale target variable\n",
    "    y_scaler = StandardScaler()\n",
    "    y = y_scaler.fit_transform(y.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    print(\"\\nPreprocessing Information:\")\n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "    print(\"\\nFeature columns:\", X.columns.tolist())\n",
    "    \n",
    "    return X, y, label_encoders, scaler, y_scaler\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(\"\\nData Split Information:\")\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Testing set shape:\", X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_preprocessed_datasets():\n",
    "    \"\"\"\n",
    "    Create and save preprocessed datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the joined dataset\n",
    "        df = pd.read_csv('Output/dataset_final_joined.csv')\n",
    "        \n",
    "        print(\"Original dataset shape:\", df.shape)\n",
    "        \n",
    "        # Preprocess the data\n",
    "        X, y, label_encoders, scaler, y_scaler = preprocess_data(df)\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "        \n",
    "        # Save preprocessed datasets\n",
    "        pd.DataFrame(X_train, columns=X.columns).to_csv('Output/X_train_preprocessed.csv', index=False)\n",
    "        pd.DataFrame(X_test, columns=X.columns).to_csv('Output/X_test_preprocessed.csv', index=False)\n",
    "        pd.DataFrame(y_train, columns=['water_req']).to_csv('Output/y_train_preprocessed.csv', index=False)\n",
    "        pd.DataFrame(y_test, columns=['water_req']).to_csv('Output/y_test_preprocessed.csv', index=False)\n",
    "        \n",
    "        print(\"\\nPreprocessed datasets have been saved to Output folder\")\n",
    "        \n",
    "        # Save some statistics about the data\n",
    "        print(\"\\nData Statistics:\")\n",
    "        print(\"Number of features:\", X.shape[1])\n",
    "        print(\"Categorical features:\", len([col for col in X.columns if X[col].dtype == 'int32']))\n",
    "        print(\"Numerical features:\", len([col for col in X.columns if X[col].dtype == 'float64']))\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, label_encoders, scaler, y_scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (2025, 21)\n",
      "\n",
      "Preprocessing Information:\n",
      "Features shape: (2025, 17)\n",
      "Target shape: (2025,)\n",
      "\n",
      "Feature columns: ['month', 'min_temp', 'max_temp', 'humidity', 'wind', 'sun', 'rad', 'rain', 'altitude', 'longitude', 'soil', 'city', 'temp_ratio', 'humidity_rad_ratio', 'season', 'crop', 'total_growing_period_avg']\n",
      "\n",
      "Data Split Information:\n",
      "Training set shape: (1620, 17)\n",
      "Testing set shape: (405, 17)\n",
      "\n",
      "Preprocessed datasets have been saved to Output folder\n",
      "\n",
      "Data Statistics:\n",
      "Number of features: 17\n",
      "Categorical features: 5\n",
      "Numerical features: 12\n",
      "\n",
      "Sample of preprocessed training data:\n",
      "      month  min_temp  max_temp  humidity      wind       sun       rad  \\\n",
      "1823      6  0.764613 -0.378992  1.148785 -0.858545 -1.165384 -1.029857   \n",
      "680       6  0.740334  1.224983 -0.783840  1.149554 -0.042480  0.186248   \n",
      "1773      5  0.407362  0.275430  0.242494  0.815312 -0.454514 -0.495650   \n",
      "1302      7 -0.293265 -0.382200  0.580433 -0.189086  1.098534  1.116396   \n",
      "1752      6  0.764613  0.894564  0.071818  1.149275 -0.128555 -0.112216   \n",
      "\n",
      "          rain  altitude  longitude  soil  city  temp_ratio  \\\n",
      "1823  1.061443 -0.969369   2.436889     0     3   -0.963252   \n",
      "680  -0.483608  1.630116  -0.755230     0     2    0.069760   \n",
      "1773 -0.112808 -0.981997  -0.003415     3     0   -0.261177   \n",
      "1302 -0.786068 -0.981874  -0.003415     3     0   -0.090357   \n",
      "1752 -0.452466 -0.981997  -0.003415     3     0   -0.159312   \n",
      "\n",
      "      humidity_rad_ratio  season  crop  total_growing_period_avg  \n",
      "1823            1.210806       2     2                 -1.100100  \n",
      "680            -0.703142       2     2                 -1.100100  \n",
      "1773            0.229982       2     4                  1.308227  \n",
      "1302           -0.149539       1     4                  1.308227  \n",
      "1752           -0.046958       2     4                  1.308227  \n",
      "\n",
      "Sample of preprocessed target values:\n",
      "   water_req\n",
      "0  -0.979141\n",
      "1   0.946057\n",
      "2  -1.024763\n",
      "3  -0.575307\n",
      "4   0.219350\n"
     ]
    }
   ],
   "source": [
    "# Execute preprocessing\n",
    "X_train, X_test, y_train, y_test, label_encoders, scaler, y_scaler = create_preprocessed_datasets()\n",
    "\n",
    "# Print sample of preprocessed data\n",
    "print(\"\\nSample of preprocessed training data:\")\n",
    "print(pd.DataFrame(X_train, columns=X_train.columns).head())\n",
    "print(\"\\nSample of preprocessed target values:\")\n",
    "print(pd.DataFrame(y_train, columns=['water_req']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline training and evaluation\n",
    "\n",
    "def create_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Create, train and evaluate multiple models for water requirement prediction\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Random Forest Model\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('rf', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    rf_params = {\n",
    "        'rf__n_estimators': [100, 200, 300],\n",
    "        'rf__max_depth': [10, 20, 30],\n",
    "        'rf__min_samples_split': [2, 5],\n",
    "        'rf__min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    rf_grid = GridSearchCV(\n",
    "        rf_pipeline,\n",
    "        rf_params,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    models['random_forest'] = rf_grid.best_estimator_\n",
    "    \n",
    "    # 2. Neural Network (Scikit-learn MLPRegressor)\n",
    "    mlp_pipeline = Pipeline([\n",
    "        ('mlp', MLPRegressor(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    mlp_params = {\n",
    "        'mlp__hidden_layer_sizes': [(50,), (100,), (50, 25)],\n",
    "        'mlp__activation': ['relu', 'tanh'],\n",
    "        'mlp__learning_rate_init': [0.001, 0.01]\n",
    "    }\n",
    "    \n",
    "    mlp_grid = GridSearchCV(\n",
    "        mlp_pipeline,\n",
    "        mlp_params,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training Neural Network (MLPRegressor) model...\")\n",
    "    mlp_grid.fit(X_train, y_train)\n",
    "    models['neural_network_sklearn'] = mlp_grid.best_estimator_\n",
    "    \n",
    "    # 3. Deep Neural Network (TensorFlow)\n",
    "    def create_deep_model(input_dim):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_dim=input_dim),\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                     loss='mse',\n",
    "                     metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    print(\"Training Deep Neural Network (TensorFlow) model...\")\n",
    "    dnn_model = create_deep_model(X_train.shape[1])\n",
    "    dnn_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    models['deep_neural_network'] = dnn_model\n",
    "    \n",
    "    # Evaluate all models\n",
    "    for name, model in models.items():\n",
    "        if name == 'deep_neural_network':\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name] = {\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'r2': r2_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "def print_results(results):\n",
    "    \"\"\"\n",
    "    Print evaluation metrics for all models\n",
    "    \"\"\"\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "        print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "        print(f\"R² Score: {metrics['r2']:.4f}\")\n",
    "\n",
    "def predict_water_requirements(model, scaler, y_scaler, features):\n",
    "    \"\"\"\n",
    "    Make predictions using the best model\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        scaled_pred = model.predict(scaled_features, verbose=0)\n",
    "    else:\n",
    "        scaled_pred = model.predict(scaled_features)\n",
    "    \n",
    "    # Inverse transform prediction\n",
    "    prediction = y_scaler.inverse_transform(scaled_pred.reshape(-1, 1))\n",
    "    \n",
    "    return prediction.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training and evaluation...\n",
      "Training Random Forest model...\n",
      "Training Neural Network (MLPRegressor) model...\n",
      "Training Deep Neural Network (TensorFlow) model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS VIVOBOOK\\envs\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "random_forest:\n",
      "MSE: 0.0037\n",
      "RMSE: 0.0612\n",
      "MAE: 0.0098\n",
      "R² Score: 0.9962\n",
      "\n",
      "neural_network_sklearn:\n",
      "MSE: 0.0008\n",
      "RMSE: 0.0287\n",
      "MAE: 0.0159\n",
      "R² Score: 0.9992\n",
      "\n",
      "deep_neural_network:\n",
      "MSE: 0.0155\n",
      "RMSE: 0.1246\n",
      "MAE: 0.0871\n",
      "R² Score: 0.9843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate models\n",
    "print(\"Starting model training and evaluation...\")\n",
    "models, results = create_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best performing model: neural_network_sklearn\n",
      "\n",
      "Sample Predictions (first 5 test samples):\n",
      "Predicted water requirements: [219.08947948  50.85579111 185.82848669  10.39267439   0.40965692]\n",
      "Actual water requirements: [2.2113e+02 5.1130e+01 1.8570e+02 1.0970e+01 6.0000e-02]\n"
     ]
    }
   ],
   "source": [
    "# Example of making predictions with the best model\n",
    "# Find best model based on R² score\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['r2'])[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "\n",
    "# Example prediction using first few samples from test set\n",
    "sample_features = X_test[10:15]\n",
    "\n",
    "# Make predictions based on model type\n",
    "if isinstance(best_model, tf.keras.Model):\n",
    "    scaled_preds = best_model.predict(sample_features, verbose=0)\n",
    "else:\n",
    "    scaled_preds = best_model.predict(sample_features).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "predictions = y_scaler.inverse_transform(scaled_preds)\n",
    "actual_values = y_scaler.inverse_transform(y_test[10:15].reshape(-1, 1))\n",
    "\n",
    "print(\"\\nSample Predictions (first 5 test samples):\")\n",
    "print(\"Predicted water requirements:\", predictions.ravel())\n",
    "print(\"Actual water requirements:\", actual_values.ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
