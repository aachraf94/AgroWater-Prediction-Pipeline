{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing pipeline 1\n",
    "def merge_rows_with_wheat_shift(df):\n",
    "    \"\"\"\n",
    "    First shifts rows where wheat appears in longitude, then merges all pairs of rows.\n",
    "    Properly preserves all values during the shifting process.\n",
    "    \"\"\"\n",
    "    processed_rows = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df) - 1:  # Process pairs of rows\n",
    "        # Get current pair of rows\n",
    "        numeric_row = df.iloc[i].copy()\n",
    "        categorical_row = df.iloc[i + 1].copy()\n",
    "        \n",
    "        # Check if this is a wheat row that needs shifting\n",
    "        if pd.notna(categorical_row['longitude']) and str(categorical_row['longitude']).strip().lower() == 'wheat':\n",
    "            # For wheat rows:\n",
    "            # 1. Move the values one column to the right\n",
    "            categorical_row['city'] = categorical_row['soil']\n",
    "            categorical_row['soil'] = categorical_row['crop']\n",
    "            categorical_row['crop'] = 'WHEAT'  # Set crop to WHEAT\n",
    "            \n",
    "            # 2. Keep the numeric longitude from the first row if it exists\n",
    "            if pd.isna(numeric_row['longitude']):\n",
    "                numeric_row['longitude'] = np.nan  # Set as missing value when no longitude exists\n",
    "        else:\n",
    "            # For non-wheat rows:\n",
    "            # If there's a longitude in the categorical row, use it\n",
    "            if pd.notna(categorical_row['longitude']):\n",
    "                numeric_row['longitude'] = categorical_row['longitude']\n",
    "        \n",
    "        # Merge categorical values into the numeric row\n",
    "        categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "        for col in categorical_cols:\n",
    "            if pd.notna(categorical_row[col]):\n",
    "                numeric_row[col] = categorical_row[col].strip()\n",
    "        \n",
    "        processed_rows.append(numeric_row)\n",
    "        i += 2\n",
    "    \n",
    "    # If there's a lone last row, add it\n",
    "    if i == len(df) - 1:\n",
    "        processed_rows.append(df.iloc[-1])\n",
    "    \n",
    "    result_df = pd.DataFrame(processed_rows, columns=df.columns)\n",
    "    \n",
    "    # Convert longitude to numeric, replacing any remaining NaN with mean\n",
    "    result_df['longitude'] = pd.to_numeric(result_df['longitude'], errors='coerce')\n",
    "    result_df['longitude'] = result_df['longitude'].fillna(result_df['longitude'].mean())\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def clean_categorical_values(df):\n",
    "    \"\"\"Standardizes categorical values by converting to uppercase and removing extra spaces.\"\"\"\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles missing values in both numeric and categorical columns.\"\"\"\n",
    "    # For numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city']\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Creates new features from existing data.\"\"\"\n",
    "    # Temperature ratio (with handling for zero values)\n",
    "    df['temp_ratio'] = df['Max Temp'] / df['Min Temp'].replace(0, np.nan)\n",
    "    df['temp_ratio'] = df['temp_ratio'].fillna(df['temp_ratio'].median())\n",
    "    \n",
    "    # Humidity/radiation ratio\n",
    "    df['humidity_rad_ratio'] = df['Humidity'] / df['Rad'].replace(0, np.nan)\n",
    "    df['humidity_rad_ratio'] = df['humidity_rad_ratio'].fillna(df['humidity_rad_ratio'].median())\n",
    "    \n",
    "    # Season mapping\n",
    "    season_mapping = {\n",
    "        'DECEMBER': 'WINTER', 'JANUARY': 'WINTER', 'FEBRUARY': 'WINTER',\n",
    "        'MARCH': 'SPRING', 'APRIL': 'SPRING', 'MAY': 'SPRING',\n",
    "        'JUNE': 'SUMMER', 'JULY': 'SUMMER', 'AUGUST': 'SUMMER',\n",
    "        'SEPTEMBER': 'AUTUMN', 'OCTOBER': 'AUTUMN', 'NOVEMBER': 'AUTUMN'\n",
    "    }\n",
    "    df['season'] = df['month'].map(season_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_and_encode(df):\n",
    "    \"\"\"Normalizes numeric features and encodes categorical variables.\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Normalize numeric columns\n",
    "    numeric_cols = ['water req', 'Min Temp', 'Max Temp', 'Humidity', 'Wind', \n",
    "                   'Sun', 'Rad', 'Rain', 'altitude', 'latitude', 'longitude',\n",
    "                   'temp_ratio', 'humidity_rad_ratio']\n",
    "    \n",
    "    # Create and fit scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    categorical_cols = ['month', 'crop', 'soil', 'city', 'season']\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_encoded[col] = encoders[col].fit_transform(df_encoded[col])\n",
    "    \n",
    "    return df_encoded, encoders\n",
    "\n",
    "def process_dataset1(input_file, output_dir):\n",
    "    \"\"\"Main function to process the dataset.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Apply the processing pipeline\n",
    "    df = merge_rows_with_wheat_shift(df)\n",
    "    df = clean_categorical_values(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Save the preprocessed but non-normalized version\n",
    "    df.to_csv(os.path.join(output_dir, 'dataset_1_preprocessed.csv'), index=False)\n",
    "    \n",
    "    # Create normalized version\n",
    "    df_normalized, encoders = normalize_and_encode(df)\n",
    "    \n",
    "    # Save the normalized version\n",
    "    df_normalized.to_csv(os.path.join(output_dir, 'dataset_1_normalized.csv'), index=False)\n",
    "    \n",
    "    return df, df_normalized, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du pipeline 1\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_file = r\"Datasets2\\dataset1\\data_plants.csv\" \n",
    "    \n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    # Unpack all three returned values\n",
    "    df_raw, df_normalized, encoders = process_dataset1(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation_id    0\n",
      "time             0\n",
      "water            0\n",
      "hour             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#import dataset 2\n",
    "df2 = pd.read_csv(\"Datasets2/dataset2/tomates.csv\")\n",
    "\n",
    "# is there missing values?\n",
    "print(df2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing pipeline 2\n",
    "def convert_timestamp(df):\n",
    "    \"\"\"Converts Unix timestamps to datetime and extracts temporal features.\"\"\"\n",
    "    # Convert Unix timestamp to datetime\n",
    "    df['datetime'] = pd.to_datetime(df['time'], unit='s')\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Drop original time column and datetime (keep date as string)\n",
    "    df['date'] = df['date'].astype(str)\n",
    "    df = df.drop(['time', 'datetime'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_hour(df):\n",
    "    \"\"\"Categorizes hours into periods of the day.\"\"\"\n",
    "    conditions = [\n",
    "        (df['hour'] >= 5) & (df['hour'] < 12),\n",
    "        (df['hour'] >= 12) & (df['hour'] < 17),\n",
    "        (df['hour'] >= 17) & (df['hour'] < 21),\n",
    "        (df['hour'] >= 21) | (df['hour'] < 5)\n",
    "    ]\n",
    "    periods = ['MORNING', 'AFTERNOON', 'EVENING', 'NIGHT']\n",
    "    \n",
    "    df['day_period'] = np.select(conditions, periods, default='UNKNOWN')\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handles any missing values in the dataset.\"\"\"\n",
    "    # For numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # For categorical columns (if any were created)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else \"UNKNOWN\"\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_and_encode(df):\n",
    "    \"\"\"Normalizes numeric features while preserving categorical ones.\"\"\"\n",
    "    # Identify numeric columns to normalize\n",
    "    numeric_cols = ['water', 'hour']\n",
    "    \n",
    "    # Create and fit scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # encode categorical columns\n",
    "    categorical_cols = ['day_period']\n",
    "    encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df[col] = encoders[col].fit_transform(df[col])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def process_dataset2(input_file, output_dir):\n",
    "    \"\"\"Main function to process the tomato dataset.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Try to read first few lines to check for header\n",
    "        with open(input_file, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        \n",
    "        # If first line contains header-like content, skip it\n",
    "        if 'time' in first_line.lower() or 'simulation' in first_line.lower():\n",
    "            df = pd.read_csv(input_file, skiprows=1, names=['simulation_id', 'time', 'water', 'hour'])\n",
    "        else:\n",
    "            df = pd.read_csv(input_file, names=['simulation_id', 'time', 'water', 'hour'])\n",
    "        \n",
    "        # Apply the processing pipeline\n",
    "        df = convert_timestamp(df)\n",
    "        df = categorize_hour(df)\n",
    "        df = handle_missing_values(df)\n",
    "        \n",
    "        # Save the preprocessed but non-normalized version\n",
    "        df.to_csv(os.path.join(output_dir, 'dataset_2_preprocessed.csv'), index=False)\n",
    "        \n",
    "        # Create normalized version\n",
    "        df_normalized, scaler = normalize_and_encode(df)\n",
    "        \n",
    "        # Save the normalized version\n",
    "        df_normalized.to_csv(os.path.join(output_dir, 'dataset_2_normalized.csv'), index=False)\n",
    "        \n",
    "        return df, df_normalized, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du pipeline 2\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset2/tomates.csv\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    # Process the dataset\n",
    "    df_raw, df_normalized, scaler = process_dataset2(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Constants\n",
    "REQUIRED_COLUMNS = {\n",
    "    'growing_period': ['Crop', 'Total growing period (days)'],\n",
    "    'water_need': ['Crop', 'Crop water need (mm/total growing period)']\n",
    "}\n",
    "\n",
    "def split_range(range_str: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Split a range string formatted as \"min-max\" into min and max values as floats.\n",
    "    \n",
    "    Args:\n",
    "        range_str: String containing the range in format \"min-max\"\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (min_value, max_value) as floats. Returns (NaN, NaN) if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle various formats and clean the input\n",
    "        range_str = str(range_str).strip().replace(' ', '')\n",
    "        \n",
    "        # Handle single values\n",
    "        if range_str.replace('.', '').isdigit():\n",
    "            value = float(range_str)\n",
    "            return value, value\n",
    "            \n",
    "        # Handle range values\n",
    "        if '-' in range_str:\n",
    "            parts = range_str.split('-')\n",
    "            if len(parts) == 2:\n",
    "                min_val = float(parts[0])\n",
    "                max_val = float(parts[1])\n",
    "                # Ensure min <= max\n",
    "                if min_val <= max_val:\n",
    "                    return min_val, max_val\n",
    "                \n",
    "        return np.nan, np.nan\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error processing range '{range_str}': {str(e)}\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def process_ranges(df: pd.DataFrame, col: str, new_min_col: str, new_max_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a column containing range strings into separate min and max columns.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        logging.error(f\"Column '{col}' not found in DataFrame\")\n",
    "        return df\n",
    "        \n",
    "    ranges = df[col].apply(split_range)\n",
    "    df[new_min_col] = ranges.apply(lambda x: x[0])\n",
    "    df[new_max_col] = ranges.apply(lambda x: x[1])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def expand_crop_names(crop_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split compound crop names into individual crops.\n",
    "    \n",
    "    Args:\n",
    "        crop_name: String containing possibly multiple crop names separated by '/'\n",
    "        \n",
    "    Returns:\n",
    "        List of individual crop names\n",
    "    \"\"\"\n",
    "    return [name.strip().upper() for name in crop_name.split('/')]\n",
    "\n",
    "def process_dataset3(input_file: str, output_dir: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the FAO Website_data.xls dataset with improved crop name handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input file\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "            \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Try reading as tab-delimited file first\n",
    "        logging.info(f\"Reading input file as tab-delimited: {input_file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, sep='\\t')\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to read as tab-delimited, trying Excel format: {str(e)}\")\n",
    "            df = pd.read_excel(input_file, engine='xlrd')\n",
    "        \n",
    "        # Split the dataframe into two parts based on non-null values\n",
    "        df_period = df[['Crop', 'Total growing period (days)']].dropna(subset=['Total growing period (days)'])\n",
    "        df_water = df[['Crop', 'Crop water need (mm/total growing period)']].dropna(subset=['Crop water need (mm/total growing period)'])\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        df_period = df_period.rename(columns={\n",
    "            'Total growing period (days)': 'Total_growing_period'\n",
    "        })\n",
    "        \n",
    "        df_water = df_water.rename(columns={\n",
    "            'Crop water need (mm/total growing period)': 'Crop_water_need'\n",
    "        })\n",
    "        \n",
    "        # Clean crop names\n",
    "        df_period['Crop'] = df_period['Crop'].str.strip().str.upper()\n",
    "        df_water['Crop'] = df_water['Crop'].str.strip().str.upper()\n",
    "        \n",
    "        # Expand compound crop names\n",
    "        period_rows = []\n",
    "        for _, row in df_period.iterrows():\n",
    "            for crop in expand_crop_names(row['Crop']):\n",
    "                new_row = row.copy()\n",
    "                new_row['Crop'] = crop\n",
    "                period_rows.append(new_row)\n",
    "        df_period = pd.DataFrame(period_rows)\n",
    "        \n",
    "        water_rows = []\n",
    "        for _, row in df_water.iterrows():\n",
    "            for crop in expand_crop_names(row['Crop']):\n",
    "                new_row = row.copy()\n",
    "                new_row['Crop'] = crop\n",
    "                water_rows.append(new_row)\n",
    "        df_water = pd.DataFrame(water_rows)\n",
    "        \n",
    "        # Process range values for both dataframes\n",
    "        df_period = process_ranges(\n",
    "            df_period,\n",
    "            'Total_growing_period',\n",
    "            'Total_growing_period_min',\n",
    "            'Total_growing_period_max'\n",
    "        )\n",
    "        df_period = df_period.drop('Total_growing_period', axis=1)\n",
    "        \n",
    "        df_water = process_ranges(\n",
    "            df_water,\n",
    "            'Crop_water_need',\n",
    "            'Crop_water_need_min',\n",
    "            'Crop_water_need_max'\n",
    "        )\n",
    "        df_water = df_water.drop('Crop_water_need', axis=1)\n",
    "        \n",
    "        # Calculate averages for both dataframes\n",
    "        df_period['Total_growing_period_avg'] = df_period[\n",
    "            ['Total_growing_period_min', 'Total_growing_period_max']\n",
    "        ].mean(axis=1)\n",
    "        \n",
    "        df_water['Crop_water_need_avg'] = df_water[\n",
    "            ['Crop_water_need_min', 'Crop_water_need_max']\n",
    "        ].mean(axis=1)\n",
    "        \n",
    "        # Merge the dataframes on Crop name\n",
    "        df_combined = pd.merge(\n",
    "            df_period,\n",
    "            df_water,\n",
    "            on='Crop',\n",
    "            how='outer'\n",
    "        )\n",
    "        \n",
    "        # Sort by crop name for better readability\n",
    "        df_combined = df_combined.sort_values('Crop')\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, 'dataset_3_processed.csv')\n",
    "        df_combined.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        return df_combined\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 18:02:18,848 - INFO - Reading input file as tab-delimited: Datasets2/dataset3/Website_data.xls\n",
      "2025-02-21 18:02:18,891 - INFO - Processed data saved to: Output\\dataset_3_processed.csv\n",
      "2025-02-21 18:02:18,893 - INFO - Dataset processing completed successfully\n",
      "2025-02-21 18:02:18,895 - INFO - Processed 40 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset3/Website_data.xls\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    try:\n",
    "        df_processed = process_dataset3(input_file, output_dir)\n",
    "        if df_processed is not None:\n",
    "            logging.info(\"Dataset processing completed successfully\")\n",
    "            logging.info(f\"Processed {len(df_processed)} records\")\n",
    "        else:\n",
    "            logging.error(\"Dataset processing failed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline de traitement pour dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "CROP_TYPE_MAPPING = {\n",
    "    1: 'Paddy',\n",
    "    2: 'Ground Nuts'\n",
    "}\n",
    "\n",
    "def validate_data(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Validate the input dataframe structure and content.\n",
    "    \"\"\"\n",
    "    required_columns = [\n",
    "        'CropType', 'CropDays', 'Soil Moisture', 'Soil Temperature',\n",
    "        'Temperature', 'Humidity', 'Irrigation(Y/N)'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Check for required columns\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                logging.error(f\"Missing required column: {col}\")\n",
    "                return False\n",
    "        \n",
    "        # Validate data types\n",
    "        if not df['CropType'].dtype in ['int64', 'float64']:\n",
    "            logging.error(\"CropType column should contain numeric values\")\n",
    "            return False\n",
    "            \n",
    "        if not df['Irrigation(Y/N)'].dtype in ['int64', 'float64']:\n",
    "            logging.error(\"Irrigation column should contain numeric values\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_dataset4(input_file: str, output_dir: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the irrigation dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input file\n",
    "        if not os.path.exists(input_file):\n",
    "            raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "            \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read only the main data part (first 7 columns)\n",
    "        logging.info(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_excel(input_file, usecols=range(7))\n",
    "        \n",
    "        # Validate data structure\n",
    "        if not validate_data(df):\n",
    "            raise ValueError(\"Data validation failed\")\n",
    "        \n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Convert crop types to names\n",
    "        df['CropType'] = df['CropType'].map(CROP_TYPE_MAPPING)\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(output_dir, 'dataset_4_processed.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing dataset: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 22:23:17,446 - INFO - Reading input file: Datasets2/dataset4/Project_datasheet_2019-2020.xlsx\n",
      "2025-02-21 22:23:17,491 - INFO - Processed data saved to: Output\\dataset_4_processed.csv\n",
      "2025-02-21 22:23:17,491 - INFO - Dataset processing completed successfully\n",
      "2025-02-21 22:23:17,497 - INFO - Processed 150 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r\"Datasets2/dataset4/Project_datasheet_2019-2020.xlsx\"\n",
    "    output_dir = \"Output\"\n",
    "    \n",
    "    try:\n",
    "        df_processed = process_dataset4(input_file, output_dir)\n",
    "        if df_processed is not None:\n",
    "            logging.info(\"Dataset processing completed successfully\")\n",
    "            logging.info(f\"Processed {len(df_processed)} records\")\n",
    "        else:\n",
    "            logging.error(\"Dataset processing failed\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline execution failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
